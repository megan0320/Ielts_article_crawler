# -*- coding: utf-8 -*-
"""S_IELTS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wg-VWuq5r_MdpsQJq5SbTwwCm0goviIa
"""

from google.colab import files
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 授权登录
def login_google_drive():
  auth.authenticate_user()
  gauth = GoogleAuth()
  gauth.credentials = GoogleCredentials.get_application_default()
  drive = GoogleDrive(gauth)
  return drive
    
drive = login_google_drive()

"""# 新增區段"""

!pip install requests
!pip install beautifulsoup4

import numpy as np
import pandas as pd
import requests
import bs4
import lxml.etree as xml

URL = "https://ielts-simon.com/"
requests.get(URL)
requests.get(URL, {}).text
web_page = bs4.BeautifulSoup(requests.get(URL, {}).text, "lxml")
mv_containers = page_html.find('div', class_ = 'entry-inner')

name=mv_containers.h3.a.text

name



web_page

web_page.head.title

web_page.head.title.text

web_page.body

tags_elements = web_page.find(name="h3", attrs={"class": "entry-header"})

tags_text = [tags_elements.text]

tags_text

tags_elements = web_page.find(name="div", attrs={"class": "entry-body"})

tags_text = [tags_elements.text]

tags_text

from time import sleep
from time import time
from random import randint
from requests import get
from IPython.core.display import clear_output
from bs4 import BeautifulSoup

#Using BeautifulSoup to parse the HTML content
html_soup = BeautifulSoup(response.text, 'html.parser')
type(html_soup)

# Redeclaring the lists to store data in
names = []
contents = []
categories = []
titles = []


# Preparing the monitoring of the loop
start_time = time()
requests = 0

# headers parameter of the get()
headers = {"Accept-Language": "en-US, en;q=0.5"}


# For every page in the interval 1-4
pages = [str(i) for i in range(1,40)]
for page in pages:

  # Make a get request
  response = get('https://ielts-simon.com/ielts-help-and-english-pr/page/' + page +'/', headers = headers)

  # Pause the loop
  sleep(randint(8,15))

  # Monitor the requests
  requests += 1
  elapsed_time = time() - start_time
  print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))
  clear_output(wait = True)

  # Throw a warning for non-200 status codes
  if response.status_code != 200:
      warn('Request: {}; Status code: {}'.format(requests, response.status_code))

  # Break the loop if the number of requests is greater than expected
  if requests > 72:
      warn('Number of requests was greater than expected.')
      break

  # Parse the content of the request with BeautifulSoup
  page_html = BeautifulSoup(response.text, 'html.parser')

  # Select all the 50 movie containers from a single page
  mv_containers = page_html.find_all('div', class_ = 'entry-inner')

  # For every movie of these 50
  for container in mv_containers:
    # Scrape the name
    name = container.h3.a.text
    names.append(name)

    category = container.h3.a.text.split(':')[0]
    categories.append(category)

    if len(container.h3.a.text.split(':')) >1:
      title = container.h3.a.text.split(':')[1]
      titles.append(title)
    else :
      titles.append('')
    
    content = container.find('div', class_ = 'entry-content').text
    contents.append(content)

import pandas as pd
test_df = pd.DataFrame({'name': names,
'category' : categories, 
'title' : titles, 
'content' : contents
})
print(test_df.info())
test_df

